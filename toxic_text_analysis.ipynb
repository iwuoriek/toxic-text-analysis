{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Toxic_Text_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ux3OQSYGhNCA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pbtQgL35OXms",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "7e165088-1fde-4314-fc43-a245b0131ecb"
      },
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from keras.models import Sequential\n",
        "from keras.preprocessing.text import text_to_word_sequence, Tokenizer\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.layers import Embedding, Conv1D, MaxPooling1D, Flatten, Dense\n",
        "from keras.layers import Activation\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "import string\n",
        "import time"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Nu82RaTdi6Ge",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyS-UOv_jWCV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 238
        },
        "outputId": "cc4093d2-6df6-41f5-e3ad-1f3d5dcd7f4b"
      },
      "source": [
        "# load dataset\n",
        "root_path = 'gdrive/My Drive/NLP/dataset/ToxicAnalysis/'\n",
        "file = root_path + 'train.csv'\n",
        "df = pd.read_csv(file)\n",
        "# check for null values\n",
        "is_null = df.columns[df.isnull().any()]\n",
        "print(df[is_null].isnull().sum())\n",
        "print(df.describe())\n",
        "# extract text and labels\n",
        "comments = df['comment_text'].values\n",
        "labels = df.loc[:,['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']].values\n",
        "size = int(len(comments) * 0.3)\n",
        "comments = comments[:size]\n",
        "labels = labels[:size,:]\n",
        "comments.shape, labels.shape"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Series([], dtype: float64)\n",
            "               toxic   severe_toxic  ...         insult  identity_hate\n",
            "count  159571.000000  159571.000000  ...  159571.000000  159571.000000\n",
            "mean        0.095844       0.009996  ...       0.049364       0.008805\n",
            "std         0.294379       0.099477  ...       0.216627       0.093420\n",
            "min         0.000000       0.000000  ...       0.000000       0.000000\n",
            "25%         0.000000       0.000000  ...       0.000000       0.000000\n",
            "50%         0.000000       0.000000  ...       0.000000       0.000000\n",
            "75%         0.000000       0.000000  ...       0.000000       0.000000\n",
            "max         1.000000       1.000000  ...       1.000000       1.000000\n",
            "\n",
            "[8 rows x 6 columns]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((47871,), (47871, 6))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5gIKtwHExib1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# clean and preprocess text\n",
        "def clean_doc(doc, remove_stop_words=True):\n",
        "  cleaned_doc = list()\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  stemmer = PorterStemmer()\n",
        "  for sentence in doc:\n",
        "    token = text_to_word_sequence(sentence)\n",
        "    table = str.maketrans('', '', string.punctuation)\n",
        "    token = [word.translate(table) for word in token]\n",
        "    token = [word for word in token if word.isalpha()]\n",
        "    if remove_stop_words:\n",
        "      token = [word for word in token if not word in stop_words]\n",
        "    token = [word for word in token if len(word) > 1]\n",
        "    token = [stemmer.stem(word) for word in token]\n",
        "    token = ' '.join(token)\n",
        "    cleaned_doc.append(token)\n",
        "  return cleaned_doc   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iAsH6WL8XGE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\" \n",
        "this function splits data to return either all non-toxic samples\n",
        "or samples identified in one or more toxic categories\n",
        "\"\"\"\n",
        "def get_toxic_data(x, y, for_toxic=True):\n",
        "  label = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "  new_x = list()\n",
        "  new_y = list()\n",
        "  for idx in range(len(x)):\n",
        "    if for_toxic and 1 in y[idx,:]:\n",
        "      new_x.append(x[idx,:])\n",
        "      new_y.append(y[idx,:])\n",
        "    if not for_toxic and not 1 in y[idx,:]:\n",
        "      new_x.append(x[idx,:])\n",
        "      new_y.append(y[idx,:])\n",
        "  return np.array(new_x), np.array(new_y)\n",
        "\n",
        "# get sample count for each label\n",
        "def get_count(y):\n",
        "  label = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "  for idx in range(len(label)):\n",
        "    count = [i for i in y[:,idx] if i == 1]\n",
        "    print(f'{label[idx]} comments: {len(count)}')\n",
        "  x, toxic = get_toxic_data(y, y)\n",
        "  print(f'overall toxic comments: {len(toxic)}')\n",
        "  print(f'overall non-toxic comments: {len(y) - len(toxic)}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hd3DGmS8X8Zo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "12145597-d392-4d2e-ef6a-e3b2df813ebf"
      },
      "source": [
        "# call function to perform cleaning on text\n",
        "train = clean_doc(comments)\n",
        "train = np.array(train)\n",
        "target = labels\n",
        "train.shape, target.shape"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((47871,), (47871, 6))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D7diEY8gdohQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "68b59937-6a66-4fbf-ca42-ac110754c4d6"
      },
      "source": [
        "# tokenize text and generate word vocabulary/dictionary\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(train)\n",
        "encoded = tokenizer.texts_to_sequences(train)\n",
        "vocab = tokenizer.word_index\n",
        "max_length = max([len(s.split()) for s in train])\n",
        "feature_vec = pad_sequences(encoded, maxlen=max_length, padding='post')\n",
        "x_train, x_test, y_train, y_test = train_test_split(feature_vec, target, test_size=0.2, random_state=7)\n",
        "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((38296, 1250), (38296, 6), (9575, 1250), (9575, 6))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3onOtp8nRMm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "outputId": "a067b774-d485-42b8-a023-20c421026070"
      },
      "source": [
        "# this section provides information of number of comments for each individual target\n",
        "print(\"Full Labels\")\n",
        "get_count(target)\n",
        "print(\"\\nTrain Labels\")\n",
        "get_count(y_train)\n",
        "print(\"\\nTest Label\")\n",
        "get_count(y_test)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Full Labels\n",
            "toxic comments: 4692\n",
            "severe_toxic comments: 502\n",
            "obscene comments: 2537\n",
            "threat comments: 162\n",
            "insult comments: 2357\n",
            "identity_hate comments: 426\n",
            "overall toxic comments: 4950\n",
            "overall non-toxic comments: 42921\n",
            "\n",
            "Train Labels\n",
            "toxic comments: 3726\n",
            "severe_toxic comments: 392\n",
            "obscene comments: 2001\n",
            "threat comments: 129\n",
            "insult comments: 1879\n",
            "identity_hate comments: 342\n",
            "overall toxic comments: 3928\n",
            "overall non-toxic comments: 34368\n",
            "\n",
            "Test Label\n",
            "toxic comments: 966\n",
            "severe_toxic comments: 110\n",
            "obscene comments: 536\n",
            "threat comments: 33\n",
            "insult comments: 478\n",
            "identity_hate comments: 84\n",
            "overall toxic comments: 1022\n",
            "overall non-toxic comments: 8553\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5ennjcUdN5_w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# CNN model params\n",
        "vocab_size = len(vocab) + 1\n",
        "output_dim = 30\n",
        "n_target = 6\n",
        "num_epochs = 100\n",
        "batch_size=256\n",
        "activation = 'sigmoid'\n",
        "loss = 'binary_crossentropy'\n",
        "optimizer = 'adam'\n",
        "\n",
        "# CNN model\n",
        "model = Sequential()\n",
        "model.add(Embedding(vocab_size, output_dim, input_length=max_length))\n",
        "model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Conv1D(filters=128, kernel_size=5, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Conv1D(filters=128, kernel_size=7, activation='relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "model.add(Flatten())\n",
        "model.add(Dense(300, activation='relu'))\n",
        "model.add(Dense(n_target, activation=activation))\n",
        "print(model.summary())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmJ7wSVl3qy5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 445
        },
        "outputId": "83c61867-7801-4a11-d3ea-7cf514408a24"
      },
      "source": [
        "# train model on dataset\n",
        "model.compile(loss=loss, optimizer=optimizer, metrics=['categorical_accuracy'])\n",
        "model.fit(x_train, y_train, epochs=num_epochs, batch_size=batch_size, verbose=0)\n",
        "loss, acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print(f'Loss: {loss}  -  Acc: {acc}')"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:793: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3657: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1033: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1020: The name tf.assign is deprecated. Please use tf.compat.v1.assign instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3005: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:190: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:197: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:207: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:216: The name tf.is_variable_initialized is deprecated. Please use tf.compat.v1.is_variable_initialized instead.\n",
            "\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:223: The name tf.variables_initializer is deprecated. Please use tf.compat.v1.variables_initializer instead.\n",
            "\n",
            "Loss: 0.2656205026998834  -  Acc: 0.9601044386422977\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jFfAQl4Dclb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# this function generates an evaluation report for the model\n",
        "def evaluate(x_test, y_test, model):\n",
        "  label = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
        "  before = time.time()\n",
        "  y_pred = model.predict(x_test)\n",
        "  after = time.time()\n",
        "  y_pred = np.round(y_pred)\n",
        "  report = classification_report(y_test, y_pred, target_names=label)\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  num_pred = accuracy_score(y_test, y_pred, normalize=False)\n",
        "\n",
        "  print(report)\n",
        "  print(f'Accuracy: {accuracy * 100}%')\n",
        "  print(f'Correctly Predicted: {num_pred}/{len(y_pred)}')\n",
        "  print(f'Inference Time: {np.round(after - before, 2)} seconds')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VS2CuV3LvOb",
        "colab_type": "code",
        "outputId": "824f7206-5044-4606-820a-04b3e71e36db",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print('Report for all samples with one or more toxic labels')\n",
        "x_toxic, y_toxic = get_toxic_data(x_test, y_test, for_toxic=True)\n",
        "evaluate(x_toxic, y_toxic, model)\n",
        "print('\\n')\n",
        "print('Report for all non-toxic smaples')\n",
        "x_non_toxic, y_non_toxic = get_toxic_data(x_test, y_test, for_toxic=False)\n",
        "evaluate(x_non_toxic, y_non_toxic, model)\n",
        "print('\\n')\n",
        "print('Report for all samples')\n",
        "evaluate(x_test, y_test, model)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Report for all samples with one or more toxic labels\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        toxic       0.97      0.68      0.80       966\n",
            " severe_toxic       0.45      0.35      0.39       110\n",
            "      obscene       0.84      0.65      0.73       536\n",
            "       threat       0.11      0.03      0.05        33\n",
            "       insult       0.64      0.59      0.61       478\n",
            "identity_hate       0.38      0.31      0.34        84\n",
            "\n",
            "    micro avg       0.80      0.61      0.69      2207\n",
            "    macro avg       0.57      0.43      0.49      2207\n",
            " weighted avg       0.80      0.61      0.69      2207\n",
            "  samples avg       0.56      0.55      0.52      2207\n",
            "\n",
            "Accuracy: 20.84148727984344%\n",
            "Correctly Predicted: 213/1022\n",
            "Inference Time: 0.16 seconds\n",
            "\n",
            "\n",
            "Report for all non-toxic smaples\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        toxic       0.00      0.00      0.00         0\n",
            " severe_toxic       0.00      0.00      0.00         0\n",
            "      obscene       0.00      0.00      0.00         0\n",
            "       threat       0.00      0.00      0.00         0\n",
            "       insult       0.00      0.00      0.00         0\n",
            "identity_hate       0.00      0.00      0.00         0\n",
            "\n",
            "    micro avg       0.00      0.00      0.00         0\n",
            "    macro avg       0.00      0.00      0.00         0\n",
            " weighted avg       0.00      0.00      0.00         0\n",
            "  samples avg       0.00      0.00      0.00         0\n",
            "\n",
            "Accuracy: 97.36934408979306%\n",
            "Correctly Predicted: 8328/8553\n",
            "Inference Time: 0.82 seconds\n",
            "\n",
            "\n",
            "Report for all samples\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "        toxic       0.74      0.68      0.71       966\n",
            " severe_toxic       0.45      0.35      0.39       110\n",
            "      obscene       0.79      0.65      0.71       536\n",
            "       threat       0.07      0.03      0.04        33\n",
            "       insult       0.56      0.59      0.57       478\n",
            "identity_hate       0.36      0.31      0.33        84\n",
            "\n",
            "    micro avg       0.68      0.61      0.64      2207\n",
            "    macro avg       0.50      0.43      0.46      2207\n",
            " weighted avg       0.67      0.61      0.64      2207\n",
            "  samples avg       0.06      0.06      0.06      2207\n",
            "\n",
            "Accuracy: 89.20104438642298%\n",
            "Correctly Predicted: 8541/9575\n",
            "Inference Time: 0.95 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bW40bZhdOd31",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}